{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import pandas as pd\n",
    "import pdb\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from IPython.core import display as ICD\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_rows', 159)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_output():\n",
    "    print(\"Subject Notes\")\n",
    "    ICD.display(df_clean.sort_index())\n",
    "    \n",
    "    print(\"\\nVolume and file count report\")\n",
    "    ICD.display(bids_df)\n",
    "    \n",
    "    print(\"\\nSubjects with only 3 runs: \".format(bids_df_3.index.values))\n",
    "    ICD.display(bids_df[bids_df[\"func_train_ct\"] == 3])\n",
    "    \n",
    "    print(\"\\nSubjects with 2 or less runs: \")\n",
    "    ICD.display(bids_df[bids_df[\"func_train_ct\"] < 3])\n",
    "    \n",
    "    \n",
    "    missing_df = bids_df[(bids_df == 0).any(axis=\"columns\")]\n",
    "    \n",
    "    print(\"\\nSubjects missing files: \")\n",
    "    ICD.display(missing_df)\n",
    "    \n",
    "    \n",
    "def dmc_report():\n",
    "    ## Report Output\n",
    "    print(\"BBx Session 1 DICOM info:\")\n",
    "    print(\"\\nUnique DICOM directories found: {}\".format(s1_sub_dcm_ct))\n",
    "    #print(\"\\nScan notes subject count: {} \\tUnique DICOM directories found: {}\".format(s1_sub_exp_ct, s1_sub_dcm_ct))\n",
    "    #print(\"DICOM IDs available: \\t\",sorted(list(map(int, s1_sub_ids ))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicom_check():\n",
    "    # Get DICOM id list\n",
    "    s1_dcms = [x.split(\"/\")[-1].split(\"-\")[1].lstrip(\"0\") for x in\n",
    "                 glob.glob(os.path.join(bids_path, \"sourcedata/DICOM/ses-1/sub-*\"))]\n",
    "    s1_dcms = np.unique(np.array(s1_dcms)).tolist()\n",
    "    # get expected id list from notes\n",
    "    s1_sub_ids = [x.split(\"_\")[1].lstrip('0') for x in df_clean.index.values]\n",
    "    s1_sub_ids = np.unique(np.array(s1_sub_ids)).tolist()\n",
    "\n",
    "    # get total count for DICOM and expected id lists\n",
    "    s1_sub_exp_ct = len(s1_sub_ids)\n",
    "    s1_sub_dcm_ct = len(s1_dcms)\n",
    "\n",
    "    # return the unique values in ar1 that are not in ar2\n",
    "\n",
    "    # ids found in dicom directory but not id list\n",
    "    s1_mia_id = np.setdiff1d(s1_dcms, s1_sub_ids)\n",
    "\n",
    "    # ids missing from dicom directories\n",
    "    s1_mia_dcm = np.setdiff1d(s1_sub_ids, s1_dcms)\n",
    "\n",
    "    s1_mia_id = s1_mia_id.tolist()\n",
    "    s1_mia_dcm = s1_mia_dcm.tolist()\n",
    "\n",
    "    # look at scan notes for any missing DICOM ids\n",
    "    print(\"NOTES ON MISSING SUBJECTS: \\n\")\n",
    "    for id_ in s1_mia_dcm:\n",
    "        bbx_id = \"bbx_{:03d}\".format(int(id_))\n",
    "        try:\n",
    "            print(\"{}, notes: \\n{}\\n\".format(bbx_id, df_clean.loc[bbx_id]))\n",
    "        except:\n",
    "            print(\"Missing scan notes for {} \\n\".format(bbx_id))\n",
    "\n",
    "\n",
    "\n",
    "    dcm_report()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    bids_path = 'tst'\n",
    "    \n",
    "    # setup report tsv\n",
    "    scan_file =os.path.join(bids_path,\"code/qc_report_s1.tsv\")\n",
    "    report = pd.read_csv(scan_file,sep=\"\\t\",index_col = \"subject\")\n",
    "    \n",
    "    # get scan notes\n",
    "    notes_path=os.path.join(bids_path, 'code/w1_notes.csv')\n",
    "    df_w1_notes=pd.read_csv(notes_path, encoding='latin-1')\n",
    "    df_clean=df_w1_notes[['participantID', 'w1scan_scannotes']]\n",
    "\n",
    "    df_clean.set_index(\"participantID\", inplace=True)\n",
    "    df_clean.index = df_clean.index.str.lower()\n",
    "    df_clean = df_clean.drop(['participant id (bbx_###)'])\n",
    "\n",
    "\n",
    "    report_output()\n",
    "    \n",
    "    # write file\n",
    "    #bids_df.to_csv(os.path.join(bids_path, \"derivatives/quality_check/ses-1_bids.tsv\"), sep=\"\\t\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_dict():\n",
    "    \n",
    "    # if no missing IDs found, that is good, meaning we have at least attempted to generate all of the subject directories available\n",
    "    # now we have to inspect individual directories\n",
    "    qa_dict={}\n",
    "    bids_dirs = glob.glob(os.path.join(bids_path, \"sub-*/ses-1\"))\n",
    "    for dir_path in sorted(bids_dirs):\n",
    "        # get id\n",
    "        sub_id = dir_path.split(\"/\")[-2]\n",
    "\n",
    "        # setup dict\n",
    "        if sub_id not in qa_dict:\n",
    "            qa_dict[sub_id] = {}\n",
    "\n",
    "        # gather files\n",
    "        func_runs=glob.glob(os.path.join(dir_path, \"func/*run*nii.gz\"))\n",
    "        func_rest=glob.glob(os.path.join(dir_path, \"func/*resting*nii.gz\"))\n",
    "        anat=glob.glob(os.path.join(dir_path, \"anat/*.nii.gz\"))\n",
    "        fmaps=glob.glob(os.path.join(dir_path, \"fmap/*.nii.gz\"))\n",
    "        #print(\"Analysis of {}:\".format(sub_id))\n",
    "\n",
    "\n",
    "\n",
    "        # Save the file count for each direcotry into the dictionary: \n",
    "\n",
    "        # Functional files:\n",
    "        if not func_runs:\n",
    "            qa_dict[sub_id][\"func_train_ct\"] = 0\n",
    "        else:\n",
    "            qa_dict[sub_id][\"func_train_ct\"] = len(func_runs)\n",
    "            for func in func_runs:\n",
    "\n",
    "                filename = func.split(\"/\")[-1]\n",
    "\n",
    "                fsl_cmd =\"fslnvols {}\".format(func)\n",
    "                vol=subprocess.check_output(fsl_cmd, shell=True)\n",
    "                vol=str(vol,'utf-8').strip()\n",
    "                if \"training\" in func:\n",
    "                    run_id= filename.split(\"_\")[3]\n",
    "                    expected_vol = 233\n",
    "                    col_name = run_id+\"_vol\"\n",
    "\n",
    "                elif \"rl\" in func:\n",
    "                    expected_vol = 212\n",
    "                    col_name = run_id+\"_vol\"\n",
    "                else:\n",
    "                    expected_vol = 147\n",
    "                    col_name = \"rest_vol\"\n",
    "\n",
    "                qa_dict[sub_id][col_name] = vol   \n",
    "\n",
    "\n",
    "        # Resting functional files:\n",
    "        if not func_rest:\n",
    "            qa_dict[sub_id][\"func_rest_ct\"] = 0\n",
    "        else:\n",
    "            qa_dict[sub_id][\"func_rest_ct\"] = len(func_rest)\n",
    "            for func in func_rest:\n",
    "\n",
    "                fsl_cmd =\"fslnvols {}\".format(func)\n",
    "                vol=subprocess.check_output(fsl_cmd, shell=True)\n",
    "                vol=str(vol,'utf-8').strip()\n",
    "\n",
    "                qa_dict[sub_id][\"resting\"] = vol   \n",
    "\n",
    "        # Fieldmap files:          \n",
    "        if not fmaps:\n",
    "            qa_dict[sub_id][\"fmap_ct\"] = 0\n",
    "        else:\n",
    "            qa_dict[sub_id][\"fmap_ct\"] = len(fmaps)\n",
    "\n",
    "\n",
    "        # Anatomical files:            \n",
    "        if not anat:\n",
    "            qa_dict[sub_id][\"anat_ct\"] = 0\n",
    "        else:\n",
    "            qa_dict[sub_id][\"anat_ct\"] = len(anat)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\\n> Completed dictionary build.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bids_check():\n",
    "    s1_bids = np.unique(np.array([x.split(\"/\")[-2].split(\"-\")[1].lstrip(\"0\") for x in\n",
    "             glob.glob(os.path.join(bids_path, \"sub-*/ses-1\"))])).tolist()\n",
    "    s1_bids_ct = len(s1_bids)\n",
    "\n",
    "    # subjects found in DICOM list but not in bids\n",
    "    s1_mia_bids = np.setdiff1d(s1_dcms, s1_bids)\n",
    "\n",
    "    s1_bids = sorted(list(map(int,  s1_bids)))\n",
    "    ## report\n",
    "    #print(\"BIDS Session 1 Info: \")\n",
    "    #print(\"Unique BIDS subject count: {} \\n\\n>>> Missing IDs: \\t{} \\n\".format(s1_bids_ct, s1_mia_bids))\n",
    "    #print(\"BIDS IDs found: \", s1_bids)\n",
    "    \n",
    "    qa_dict = data_dict()\n",
    "    bids_df = pd.DataFrame(qa_dict).T\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
